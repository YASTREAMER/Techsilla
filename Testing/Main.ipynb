{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7181aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm_dsnoop.c:567:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1000:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_oss.c:404:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:404:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib dlmisc.c:339:(snd_dlobj_cache_get0) Cannot open shared library libasound_module_pcm_a52.so (/home/fall3n/anaconda3/envs/Tech/lib/python3.11/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /usr/lib/libjxl.so.0.11))\n",
      "ALSA lib dlmisc.c:339:(snd_dlobj_cache_get0) Cannot open shared library libasound_module_pcm_a52.so (/home/fall3n/anaconda3/envs/Tech/lib/python3.11/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /usr/lib/libjxl.so.0.11))\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:481:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:481:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib pcm_dmix.c:1000:(snd_pcm_dmix_open) unable to open slave\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording\n",
      "* done recording\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "CHUNK = 1024*4\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"* recording\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "\n",
    "print(\"* done recording\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f74c847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "from scipy.io import wavfile\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "import noisereduce as nr\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "rate, data = wavfile.read(\"output.wav\")\n",
    "\n",
    "# if stereo, convert to mono by averaging channels\n",
    "if data.ndim > 1:\n",
    "    data = np.mean(data, axis=1)\n",
    "\n",
    "# perform noise reduction\n",
    "reduced_noise = nr.reduce_noise(y=data, sr=rate)\n",
    "\n",
    "# save output\n",
    "wavfile.write(\"mywav_reduced_noise.wav\", rate, reduced_noise.astype(np.int16))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "`return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Parishwarya and I am into machine learning.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"distil-whisper/distil-large-v3.5\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "result = pipe(\"output.wav\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Delete the model object\n",
    "del model\n",
    "del pipe\n",
    "\n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear CUDA cache if running on GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95763318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import nltk\n",
    "\n",
    "# Grammar correction model\n",
    "corrector = pipeline(\"text2text-generation\", model=\"prithivida/grammar_error_correcter_v1\")\n",
    "\n",
    "def grammar_score(sentence):\n",
    "    corrected = corrector(sentence, max_length=128)[0]['generated_text']\n",
    "    # Simple token-based edit distance\n",
    "    dist = nltk.edit_distance(sentence.split(), corrected.split())\n",
    "    max_len = max(len(sentence.split()), len(corrected.split()))\n",
    "    score = 1 - dist / max_len  # normalized accuracy score\n",
    "    return corrected, round(score * 100, 2)\n",
    "\n",
    "s = \"He is going to school.\"\n",
    "corrected, score = grammar_score(s)\n",
    "print(\"Original:\", s)\n",
    "print(\"Corrected:\", corrected)\n",
    "print(\"Grammar Accuracy Score:\", score, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee8939",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a tech interviewer and you only ask them question and do not have to say anything else and you have to ask the candidate question based on what they specialize in. You only have to ask them about tech interview question such as DSA or ML related question. If they give you information about any project that they have done then you will have to ask them question regarding that project.\"\n",
    "    ,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Hello I am Priyanshu and I am praticing Machine learning and Deep learning and I have done a project on CNN in which I used CNN to classify digits\"},\n",
    "]\n",
    "    # {\"role\": \"user\", \"content\": result[\"text\"]},\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.1, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b55d0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f63c622",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textIDK' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# prepare the model input\u001b[39;00m\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello I am Priyanshu and I am praticing Machine learning and Deep learning and I have done a project on CNN in which I used CNN to classify digits\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     {\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a tech interviewer and you only ask them question and do not have to say anything else and you have to ask the candidate question based on what they specialize in. You only have to ask them about tech interview question such as DSA or ML related question. If they give you information about any project that they have done then you will have to ask them question regarding that project. Ask the canditate around 10 questions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     ,\n\u001b[1;32m      8\u001b[0m     },\n\u001b[0;32m----> 9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:textIDK }\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     11\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     12\u001b[0m     messages,\n\u001b[1;32m     13\u001b[0m     tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     enable_thinking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# Switches between thinking and non-thinking modes. Default is True.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer([text], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'textIDK' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Hello I am Priyanshu and I am praticing Machine learning and Deep learning and I have done a project on CNN in which I used CNN to classify digits\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a tech interviewer and you only ask them question and do not have to say anything else and you have to ask the candidate question based on what they specialize in. You only have to ask them about tech interview question such as DSA or ML related question. If they give you information about any project that they have done then you will have to ask them question regarding that project. Ask the canditate around 10 questions\"\n",
    "    ,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\":textIDK }\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True)\n",
    "content = content + \"\\n\"\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n",
    "# prepare the model input\n",
    "prompt = \"Hello I am Priyanshu and I am praticing Machine learning and Deep learning and I have done a project on CNN in which I used CNN to classify digits\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a tech interviewer and you only ask them question and do not have to say anything else and you have to ask the candidate question based on what they specialize in. You only have to ask them about tech interview question such as DSA or ML related question. If they give you information about any project that they have done then you will have to ask them question regarding that project. Ask the canditate around 10 questions\"\n",
    "    ,\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\":textIDK }\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True)\n",
    "content = content + \"\\n\"\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
